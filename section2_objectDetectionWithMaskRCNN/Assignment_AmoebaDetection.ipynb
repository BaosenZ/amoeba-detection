{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/BaosenZ/amoeba-detection/blob/master/level2_objectDetectionWithMaskRCNN/Solution_AmoebaDetection.ipynb","timestamp":1653659454025}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":["# Assignment: Amoeba Detection"],"metadata":{"id":"dWfF5ROjdKuG"}},{"cell_type":"markdown","source":["This is the assignment called Amoeba Detection. The students are encouraged to fill out the code block in **\"Train custom amoeba model\"** and **\"Evaluate amoeba model\"** parts by understanding the code in \"Example: kangaroo detection\".\n","\n","Here, we use the images that are collected from our research lab to train our own custom model to detect the position and number of amoeba in the image."],"metadata":{"id":"vro9ERPIj-kO"}},{"cell_type":"markdown","source":["## Table of content\n","\n","\n","* Set up enviroment\n","* Load images dataset\n","* Data preparation and configurations\n","* Train custom amoeba model (blank in here)\n","* Evaluate amoeba model (blank in here)\n","* Inference"],"metadata":{"id":"7M77-yL-dmr2"}},{"cell_type":"markdown","metadata":{"id":"Uv718omLXu64"},"source":["# Set up environment"]},{"cell_type":"markdown","source":["We will install Mask R-CNN repo from Github (https://github.com/matterport/Mask_RCNN) and necessary packages. We need to make sure the packages we are using is the correct version."],"metadata":{"id":"-xF51fTo80NT"}},{"cell_type":"code","metadata":{"id":"DP0XHZ_2ND6y"},"source":["!pip install tensorflow==1.15.0\n","!pip install keras==2.1.6\n","!pip install h5py==2.10.0\n","!pip install scikit-image==0.16.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hmEs1lZiN5hz"},"source":["%%shell\n","# clone Mask_RCNN repo and install packages\n","git clone https://github.com/matterport/Mask_RCNN\n","cd Mask_RCNN\n","python setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqIPGkJEVsiw"},"source":["import os\n","import sys\n","import random\n","import math\n","import numpy as np\n","import skimage.io\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"./Mask_RCNN/\")\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","from mrcnn import utils\n","import mrcnn.model as modellib\n","from mrcnn import visualize\n","\n","# Import COCO config\n","sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # find local version\n","import coco\n","\n","%matplotlib inline \n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to trained weights file\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)\n","\n","# Directory of images to run detection on\n","IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load images dataset"],"metadata":{"id":"zO0KufAsfFD1"}},{"cell_type":"markdown","source":["We will clone the project from our github (https://github.com/BaosenZ/amoeba-detection.git). The dataset is included inside the github project."],"metadata":{"id":"qziLMTsTTH-T"}},{"cell_type":"code","source":["# download dataset from github\n","\n","%%shell\n","git clone https://github.com/BaosenZ/amoeba-detection.git\n"],"metadata":{"id":"TyxOr0u5u5Ci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# copy the dataset from github folder to 'content' \n","!cp -r '/content/amoeba-detection/dataset-section2/amoebaDataset/trainingDataset' '/content'\n","!cp -r '/content/amoeba-detection/dataset-section2/amoebaDataset/testDataset' '/content'"],"metadata":{"id":"B4JXR9TSu8ln"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7H8c5VhVZwi"},"source":["# upload zip of the dataset from local\n","\n","# from google.colab import files\n","# uploaded = files.upload()\n","# for fn in uploaded.keys():\n","#     print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n","\n","# !unzip trainingDataset.zip\n","# !unzip testDataset.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data preparation and configurations"],"metadata":{"id":"9zUpZ6MxdlON"}},{"cell_type":"code","metadata":{"id":"fCbodMuKJLeF"},"source":["from os import listdir\n","from xml.etree import ElementTree\n","from numpy import zeros\n","from numpy import asarray\n","from mrcnn.utils import Dataset\n","from mrcnn.config import Config\n","from mrcnn.model import MaskRCNN\n","\n","class AmoebaDataset(Dataset):\n","\tdef load_dataset(self, dataset_dir, is_train=True):\n","\t\t# Add classes\n","\t\tself.add_class(\"dataset\", 1, \"amoeba\")\n","\t\timages_dir = dataset_dir + '/images/'\n","\t\tannotations_dir = dataset_dir + '/annots/'\n","\t\tfor filename in listdir(images_dir):\n","\t\t\timage_id = filename[:-4]\n","\t\t\t# based on images number, split the training and validation data\n","\t\t\tif is_train and int(image_id) >= 170:\n","\t\t\t\tcontinue\n","\t\t\tif not is_train and int(image_id) < 170:\n","\t\t\t\tcontinue\n","\t\t\timg_path = images_dir + filename\n","\t\t\tann_path = annotations_dir + image_id + '.xml'\n","\t\t\tself.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n","\n","\t# extract bounding boxes from an annotation file\n","\tdef extract_boxes(self, filename):\n","\t\ttree = ElementTree.parse(filename)\n","\t\troot = tree.getroot()\n","\t\tboxes = list()\n","\t\tfor box in root.findall('.//bndbox'):\n","\t\t\txmin = int(box.find('xmin').text)\n","\t\t\tymin = int(box.find('ymin').text)\n","\t\t\txmax = int(box.find('xmax').text)\n","\t\t\tymax = int(box.find('ymax').text)\n","\t\t\tcoors = [xmin, ymin, xmax, ymax]\n","\t\t\tboxes.append(coors)\n","\t\twidth = int(root.find('.//size/width').text)\n","\t\theight = int(root.find('.//size/height').text)\n","\t\treturn boxes, width, height\n","\n","\t# load the masks for an image\n","\tdef load_mask(self, image_id):\n","\t\tinfo = self.image_info[image_id]\n","\t\tpath = info['annotation']\n","\t\tboxes, w, h = self.extract_boxes(path)\n","\t\tmasks = zeros([h, w, len(boxes)], dtype='uint8')\n","\t\tclass_ids = list()\n","\t\tfor i in range(len(boxes)):\n","\t\t\tbox = boxes[i]\n","\t\t\trow_s, row_e = box[1], box[3]\n","\t\t\tcol_s, col_e = box[0], box[2]\n","\t\t\tmasks[row_s:row_e, col_s:col_e, i] = 1\n","\t\t\tclass_ids.append(self.class_names.index('amoeba'))\n","\t\treturn masks, asarray(class_ids, dtype='int32')\n","\n","\tdef image_reference(self, image_id):\n","\t\tinfo = self.image_info[image_id]\n","\t\treturn info['path']\n","\n","\n","# prepare train set\n","train_set = AmoebaDataset()\n","train_set.load_dataset('trainingDataset', is_train=True)\n","train_set.prepare()\n","print('Train: %d' % len(train_set.image_ids))\n","# prepare val set\n","val_set = AmoebaDataset()\n","val_set.load_dataset('trainingDataset', is_train=False)\n","val_set.prepare()\n","print('Val: %d' % len(val_set.image_ids))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Config files allow you to separate the code from the parameters of the machine learning pipeline to help produce repeatable outcomes."],"metadata":{"id":"F2m1Vz7RTOZX"}},{"cell_type":"code","source":["# prepare config\n","class AmoebaConfig(Config):\n","\t# Give the configuration a recognizable name\n","\tNAME = \"amoeba_cfg\"\n","\t# Number of classes (including background)\n","\tNUM_CLASSES = 1 + 1 # background + 1 amoeba\n","\t# Use a small epoch since the data is simple\n","\tSTEPS_PER_EPOCH = 131\n","\tDETECTION_NMS_THRESHOLD = 0.5\n","\n","config = AmoebaConfig()\n","config.display()"],"metadata":{"id":"LTbDIzfBTPtA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GTy9wXmK8w3E"},"source":["# Train custom amoeba model (blank in here)"]},{"cell_type":"markdown","source":["In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. More information about Tensorboard is available here (https://www.tensorflow.org/tensorboard/get_started)."],"metadata":{"id":"HD8l3YFyTYk7"}},{"cell_type":"code","metadata":{"id":"gB1kiNlUAMGM"},"source":["# run tensorboard to visualize training\n","import keras\n","import os\n","root_logdir = os.path.join(os.curdir, \"my_logs\")\n","def get_run_logdir():\n","    import time\n","    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n","    return os.path.join(root_logdir, run_id)\n","\n","run_logdir = get_run_logdir()\n","print(run_logdir)\n","tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Mask R-CNN structure is described in the paper(He, K., Gkioxari, G., DollÃ¡r, P. and Girshick, R., 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision (pp. 2961-2969).) and also in my ppt."],"metadata":{"id":"6Xd0i1a5Tb0x"}},{"cell_type":"markdown","source":["Train in two stages:\n","1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n","\n","2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."],"metadata":{"id":"Dt5p3vxDgwcf"}},{"cell_type":"markdown","source":["### Fill out the blank in 'train the model' block"],"metadata":{"id":"AR_pyWu8OGun"}},{"cell_type":"code","metadata":{"id":"5aqMMXOh_XRG"},"source":["# train the model\n","model = MaskRCNN(mode='training', model_dir='./', config=config)\n","model.load_weights('Mask_RCNN/mask_rcnn_coco.h5', by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n","model.train(, , learning_rate=0.002, epochs=10, layers='', custom_callbacks=[tensorboard_cb]) # blank in here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXf0LbQz1IWk"},"source":["# Evaluate amoeba model (blank in here)\n"," \n"]},{"cell_type":"markdown","source":["Evaluate the mask rcnn model on the training, validation and test amoeba dataset"],"metadata":{"id":"iZ-g5om6B63p"}},{"cell_type":"code","source":["from os import listdir\n","from xml.etree import ElementTree\n","from numpy import zeros\n","from numpy import asarray\n","from numpy import expand_dims\n","from numpy import mean\n","from mrcnn.config import Config\n","from mrcnn.model import MaskRCNN\n","from mrcnn.utils import Dataset\n","from mrcnn.utils import compute_ap\n","from mrcnn.model import load_image_gt\n","from mrcnn.model import mold_image\n","\n","class PredictionConfig(Config):\n","\tNAME = \"amoeba_cfg\"\n","\tNUM_CLASSES = 1 + 1\n","\tGPU_COUNT = 1\n","\tIMAGES_PER_GPU = 1\n","\n","# calculate the mAP for a model on a given dataset\n","def evaluate_model(dataset, model, cfg):\n","\tAPs = list()\n","\tfor image_id in dataset.image_ids:\n","\t\timage, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)\n","\t\tscaled_image = mold_image(image, cfg)\n","\t\tsample = expand_dims(scaled_image, 0)\n","\t\tyhat = model.detect(sample, verbose=0)\n","\t\tr = yhat[0]\n","\t\tAP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n","\t\tAPs.append(AP)\n","\tmAP = mean(APs)\n","\treturn mAP\n"],"metadata":{"id":"LvMEnCcC4nmj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create config\n","cfg = PredictionConfig()\n","# define the model\n","model = MaskRCNN(mode='inference', model_dir='./', config=cfg)\n","\n","# !!!load model weights\n","# Get path to saved weights\n","# Either set a specific path or find last trained weights\n","# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n","# find last trained weights: \n","model_path = model.find_last()\n","model.load_weights(model_path, by_name=True)\n","\n","# set a specific path\n","#model.load_weights('amoeba_cfg20210206T1746/mask_rcnn_amoeba_cfg_0007.h5', by_name=True)  # change the weights path to run the code"],"metadata":{"id":"Ek-C09UD5OLZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fill out the blank in 'eval the model' block"],"metadata":{"id":"kEpFKOCIOTa9"}},{"cell_type":"code","metadata":{"id":"YjH_BfMA1CDX"},"source":["# load the train dataset\n","train_set = AmoebaDataset()\n","train_set.load_dataset('trainingDataset', is_train=True)\n","train_set.prepare()\n","print('Train: %d' % len(train_set.image_ids))\n","# load the val dataset\n","val_set = AmoebaDataset()\n","val_set.load_dataset('trainingDataset', is_train=False)\n","val_set.prepare()\n","print('Val: %d' % len(val_set.image_ids))\n","# load the test dataset\n","test_set = AmoebaDataset()\n","test_set.load_dataset('testDataset')\n","test_set.prepare()\n","print('Test: %d' % len(test_set.image_ids))\n","\n","\n","# evaluate model on training dataset\n","train_mAP = evaluate_model( , model, cfg) # blank here\n","print(\"Train mAP: %.3f\" % train_mAP)\n","# evaluate model on test dataset\n","val_mAP = evaluate_model( , model, cfg) # blank here\n","print(\"Val mAP: %.3f\" % val_mAP)\n","# evaluate model on test dataset\n","test_mAP = evaluate_model( , model, cfg) # blank here\n","print(\"Test mAP: %.3f\" % test_mAP)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# visualize training in Tensorboard\n","\n","# The tensorboard file name can be found in 'my_logs'. Then change the tensorboard file name.\n","\n","# %load_ext tensorboard\n","# %tensorboard --logdir ./my_logs/run_2022_06_20-22_07_58"],"metadata":{"id":"qEyaPhz9Txmj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ESecZKkV2_M1"},"source":["# Inference\n"]},{"cell_type":"markdown","metadata":{"id":"NQjYvkVaosFu"},"source":["## Drop inference"]},{"cell_type":"markdown","source":["The concentration of amoebae in a sample can be determined by imaging an entire drop of a particular volume of the sample and counting the number of amoebae present there. The models described above were used to analyze the images of the entire drop and count the number of bounding boxes within all images. \n","\n","There are 6 drops examples. Students can change the file location to do inference on each of the drop. "],"metadata":{"id":"2nlMT57FZBF8"}},{"cell_type":"markdown","source":["We will detect the images in testDataset folder by calling model.detect()."],"metadata":{"id":"c_-UcSMBT1Rk"}},{"cell_type":"code","source":["from matplotlib import pyplot\n","from matplotlib.patches import Rectangle\n","\n","  \n","def count1_amoeba(image, model, cfg):\n","  image = np.asanyarray(image)\n","  scaled_image = mold_image(image, cfg)\n","  sample = expand_dims(image, 0)\n","  yhat = model.detect(sample, verbose=0)[0]\n","  \n","  count=0\n","  for box,confidence in zip(yhat['rois'],yhat['scores']):\n","    if confidence >=0.96:\n","      count = count +1\n","  print(\"The number of amoeba is \", count)\n","  return count\n","\n","def save_predicted(image, model, cfg, filename):\n","  image = np.asanyarray(image)\n","  scaled_image = mold_image(image, cfg)\n","  sample = expand_dims(image, 0)\n","  yhat = model.detect(sample, verbose=0)[0]\n","  pyplot.imshow(image)\n","  ax = pyplot.gca()\n","  pyplot.axis(\"off\")\n","  for box,confidence in zip(yhat['rois'],yhat['scores']):\n","    if confidence >= 0.96:\n","      y1, x1, y2, x2 = box\n","      width, height = x2 - x1, y2 - y1\n","      rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n","      pyplot.text(x1,y1,confidence)\n","      ax.add_patch(rect)\n","\n","  pyplot.savefig(filename,bbox_inches='tight',pad_inches=0.0)\n","  pyplot.show()\n"],"metadata":{"id":"8mbAJUPZSCyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5iRklVsBn9qE"},"source":["# create folder to save inference images\n","from PIL import Image\n","if not os.path.exists(\"drop1-inference\"):\n","  os.mkdir(\"drop1-inference\")\n","\n","# find the path of original inference images\n","images_dir = \"amoeba-detection/dataset-level2/dropInference/drop1/drop1\"\n","save_root = \"drop1-inference/\"\n","# run the inference and count the amoeba\n","t=0\n","for img in listdir(images_dir):\n","  img_path = images_dir + \"/\" + img\n","  image = Image.open(img_path)\n","  image = np.asanyarray(image)\n","  save_file = save_root + img\n","  save_predicted(image, model, cfg, save_file)\n","  t=t + count1_amoeba(image,model,cfg)\n","  print(t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3YyoNMioZDZ"},"source":["# save the inference folder to local\n","\n","# !zip -r drop1-inference.zip drop1-inference\n","# from google.colab import files\n","# files.download(\"drop1-inference.zip\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4_uw7QD115NJ"},"source":["## Test dataset inference"]},{"cell_type":"code","metadata":{"id":"sD45PtNO13_5"},"source":["\n","# create folder to save inference images\n","from PIL import Image\n","if not os.path.exists(\"testDataset-pred\"):\n","  os.mkdir(\"testDataset-pred\")\n","\n","# find the path of original inference images\n","images_dir = \"amoeba-detection/dataset-level2/amoebaDataset/testDataset/images\"\n","save_root = \"testDataset-pred/\"\n","# run the inference and count the amoeba\n","t=0\n","for img in listdir(images_dir):\n","  img_path = images_dir + \"/\" + img\n","  image = Image.open(img_path)\n","  image = np.asanyarray(image)\n","  save_file = save_root + img\n","  save_predicted(image, model, cfg, save_file)\n","  t=t + count1_amoeba(image,model,cfg)\n","  print(t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lk_EIqRV2VLL"},"source":["# save the inference folder to local\n","\n","# !zip -r testDataset-pred.zip testDataset-pred\n","# from google.colab import files\n","# files.download(\"testDataset-pred.zip\")"],"execution_count":null,"outputs":[]}]}