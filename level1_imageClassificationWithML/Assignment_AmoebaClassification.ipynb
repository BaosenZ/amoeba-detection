{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajpj5fG0My9A"
      },
      "source": [
        "# Assignment: Amoeba Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_9pRl8NL8c"
      },
      "source": [
        "This is the amoeba classification assignment. The students are encouraged to fill out the code block in \"Build and train the model\" and \"Evaluate the model\" parts by understanding the code in \"Example: Clothes classification\".\n",
        "\n",
        "Here, we use the images that were collected in our research lab to train our own custom model and classify the images if they contain an amoeba or not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qxh_PToNZT5"
      },
      "source": [
        "## Table of content\n",
        "\n",
        "* Load images dataset\n",
        "* Data preparation\n",
        "* Build and train the model (blank in here)\n",
        "* Evaluate the model (blank in here)\n",
        "* Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPAnihvmRgsU"
      },
      "source": [
        "# Load images dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images dataset is loaded and we will use it to train our custom model. All of the images were collected in our research lab.  "
      ],
      "metadata": {
        "id": "zR_G2WG5EY-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efgc0DXyRfeR"
      },
      "outputs": [],
      "source": [
        "# upload zip file of the dataset from local\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "!unzip dataset-amoebaClassification.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2pG6gtQRkH0"
      },
      "outputs": [],
      "source": [
        "# download dataset from github\n",
        "\n",
        "# %%shell\n",
        "# git clone https://github.com/BaosenZ/amoeba-detection.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpenRg5MT2IY"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0zjFJ_TXoS1"
      },
      "source": [
        "In this step, we will prepare the data, including spliting it into a training, validation and test datasets, and will also normalize the datasets. Here we provide one method to prepare the dataset. More ways can be found here: https://keras.io/examples/vision/image_classification_from_scratch/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxTtsppveenE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# training data preparation\n",
        "\n",
        "# define image size, it can be modified \n",
        "img_size = 299\n",
        "# training images dataset path\n",
        "train_path = 'dataset-amoebaClassification/train' \n",
        "nub_train = len(glob(train_path + '/*/*.jpg'))\n",
        "# Create empty array, fill out the image array to newly-created array. \n",
        "X_train = np.zeros((nub_train,img_size,img_size,3),dtype=np.uint8) \n",
        "y_train = np.zeros((nub_train,),dtype=np.uint8)\n",
        "\n",
        "i = 0\n",
        "for img_path in tqdm(glob(train_path + '/*/*.jpg')):\n",
        "    # print(img_path)\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    # image resize\n",
        "    img = img.resize((img_size,img_size)) \n",
        "    # images are converted to array\n",
        "    arr = np.asarray(img)\n",
        "    # assign array\n",
        "    X_train[i, :, :, :] = arr\n",
        "    \n",
        "    if img_path.split('/')[-2] == 'amoeba':\n",
        "        # Set amoeba class as 0\n",
        "        y_train[i] = 0\n",
        "    else:\n",
        "        # Set no amoeba class as 1\n",
        "        y_train[i] = 1\n",
        "        \n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oorvTXAkq3kp"
      },
      "outputs": [],
      "source": [
        "# validation data preparation\n",
        "\n",
        "# define image size, it can be modified \n",
        "img_size = 299\n",
        "# validation images dataset path\n",
        "validation_path = 'dataset-amoebaClassification/validation' \n",
        "nub_validation = len(glob(validation_path + '/*/*.jpg'))\n",
        "# Creat empty array, fill out the image array to newly-created array. \n",
        "X_validation = np.zeros((nub_validation,img_size,img_size,3),dtype=np.uint8) \n",
        "y_validation = np.zeros((nub_validation,),dtype=np.uint8)\n",
        "\n",
        "i = 0\n",
        "for img_path in tqdm(glob(validation_path + '/*/*.jpg')):\n",
        "    # print(img_path)\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    # image resize\n",
        "    img = img.resize((img_size,img_size)) \n",
        "    # images are converted to array\n",
        "    arr = np.asarray(img)\n",
        "    # assign array\n",
        "    X_validation[i, :, :, :] = arr\n",
        "    \n",
        "    if img_path.split('/')[-2] == 'amoeba':\n",
        "        # Set cat class as 0\n",
        "        y_validation[i] = 0\n",
        "    else:\n",
        "        # Set dog class as 1\n",
        "        y_validation[i] = 1\n",
        "        \n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6b2hvOceltB"
      },
      "outputs": [],
      "source": [
        "# test data preparation\n",
        "\n",
        "img_size = 299\n",
        "test_path = 'dataset-amoebaClassification/test'\n",
        "nub_test = len(glob(test_path + '/*/*.jpg'))\n",
        "\n",
        "X_test = np.zeros((nub_test,img_size,img_size,3),dtype=np.uint8) \n",
        "y_test = np.zeros((nub_test,),dtype=np.uint8)\n",
        "\n",
        "i = 0\n",
        "for img_path in tqdm(glob(test_path + '/*/*.jpg')):\n",
        "    # print(img_path)\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((img_size,img_size))\n",
        "    arr = np.asarray(img)\n",
        "    X_test[i, :, :, :] = arr\n",
        "          \n",
        "    if img_path.split('/')[-2] == 'amoeba':\n",
        "        # Set cat class as 0\n",
        "        y_test[i] = 0\n",
        "    else:\n",
        "        # Set dog class as 1\n",
        "        y_test[i] = 1\n",
        "        \n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QDAA7mieoAH"
      },
      "outputs": [],
      "source": [
        "# Visualize the training dataset\n",
        "fig,axes = plt.subplots(3,4,figsize=(20, 20))\n",
        "\n",
        "j = 0\n",
        "for i,img in enumerate(X_train[:12]):\n",
        "    axes[i//4,j%4].imshow(img)\n",
        "    j+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ksJIfJSrRA6"
      },
      "outputs": [],
      "source": [
        "# normalize the dataset\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train_norm = (X_train - X_mean) / X_std\n",
        "X_validation_norm = (X_validation - X_mean) / X_std\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train_norm = X_train_norm[..., np.newaxis]\n",
        "X_validation_norm = X_validation_norm[..., np.newaxis]\n",
        "X_test_norm = X_test_norm[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Hy73CASCOy"
      },
      "source": [
        "# Build and train the model (blank in here)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple convolutional neural network (CNN) is used to train the model. "
      ],
      "metadata": {
        "id": "odOHP35HCpJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbT_kEdWrZwU"
      },
      "outputs": [],
      "source": [
        "# Build the model (sequential CNN model)\n",
        "from functools import partial\n",
        "\n",
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "\n",
        "model = keras.models.Sequential([                           \n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[299, 299, 3]),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=32, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=2, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUmjbFiJrcga"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill out the blank here to start training."
      ],
      "metadata": {
        "id": "Pg9HyMiY29K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model. ( , , epochs=10, validation_data=(X_validation_norm, y_validation))"
      ],
      "metadata": {
        "id": "04XEgAhtk1Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw8CUGhuSQN5"
      },
      "source": [
        "# Evaluate the model (blank in here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGy1zgY4e7Hk"
      },
      "outputs": [],
      "source": [
        "# visualize the model structure with model.summary(). Feel free to comment out the code below to visualize the model structure\n",
        "\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU4WzhMDZx-U"
      },
      "source": [
        "The test dataset is not used for training and validation, which means the images are new to the trained model. We will use this dataset to evaluate the performance of the model. The performance is acceptable because the accuracy for the test dataset is nearly the same as the accuracy for the train and validation datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fill out the blank here to finish performance evaluation"
      ],
      "metadata": {
        "id": "lW4POzpW21dY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "megn6r0QeKEV"
      },
      "outputs": [],
      "source": [
        "# Using test dataset to evaluate loss and accuracy for trained model\n",
        "results = model.evaluate( , , batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axn1bFdOZwIu"
      },
      "source": [
        "At the end of epochs, the accuary for the training and validation datasets should be close in value. This is an easy way to determine if there is overfitting or not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4uf0r4ie9Ow"
      },
      "outputs": [],
      "source": [
        "# plot accuracy vs epoch\n",
        "plt.plot(history.history['accuracy'],'r')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left') \n",
        "plt.ylim([0, 1.1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7nuvpvtULUK"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql-ieznSZ6I0"
      },
      "source": [
        "We will visualize the image and use our own judgement to see if there is an amoeba in the image or not, and then compare the outcome to the model's prediction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llMoMhl3jsKu"
      },
      "outputs": [],
      "source": [
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Visualize one image, X_test[x]. Here we choose X_test[1]. You can choose any of the images among all test dataset\n",
        "img1 = X_test[1]\n",
        "plt.imshow(img1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpB2C5ccZ_r0"
      },
      "source": [
        "We will predict the image above with model.predict() function to see if it matches our judgement. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpCYQiw5j-Y_"
      },
      "outputs": [],
      "source": [
        "# class label\n",
        "class_label = ['amoeba exist', 'no amoeba exist']\n",
        "\n",
        "# image process\n",
        "x = np.squeeze(X_test_norm[1])\n",
        "x = image.img_to_array(x)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# predict the image with model.predict()\n",
        "y_prob = model.predict(x)\n",
        "print(\"probality for each of the catogaries: \", y_prob)\n",
        "y_class = y_prob.argmax(axis=-1)\n",
        "print(\"model predict: \", class_label[y_class[0]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_AmoebaClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}