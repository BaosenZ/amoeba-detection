{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajpj5fG0My9A"
      },
      "source": [
        "# Assignment: Amoeba Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce_9pRl8NL8c"
      },
      "source": [
        "This is the amoeba classification assignment. The students are encouraged to fill out the code block in \"Build and train the model\" and \"Evaluate the model\" parts by understanding the code in \"Example: Clothes classification\".\n",
        "\n",
        "Here, we use the images that were collected in our research lab to train our own custom model and classify the images if they contain an amoeba or not. The dataset is not large enough, we use two techniques to make our experiment reproducible: data augmentation and repeat the experiment multiple times. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qxh_PToNZT5"
      },
      "source": [
        "## Table of content\n",
        "\n",
        "* Load images dataset\n",
        "* Data preparation\n",
        "* Build and train the model (blank in here)\n",
        "* Evaluate the model (blank in here)\n",
        "* Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPAnihvmRgsU"
      },
      "source": [
        "# Load images dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR_G2WG5EY-X"
      },
      "source": [
        "The images dataset is loaded and we will use them to train our custom model. All of the images are collected in our lab.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-l0MLVJod-g",
        "outputId": "a67a5e32-0c05-405d-87f1-509fcc84a856"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'amoeba-detection'...\n",
            "remote: Enumerating objects: 2109, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 2109 (delta 13), reused 23 (delta 7), pack-reused 2075\u001b[K\n",
            "Receiving objects: 100% (2109/2109), 315.32 MiB | 26.81 MiB/s, done.\n",
            "Resolving deltas: 100% (338/338), done.\n",
            "Checking out files: 100% (3056/3056), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# download dataset from github\n",
        "\n",
        "%%shell\n",
        "git clone https://github.com/BaosenZ/amoeba-detection.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgmzCrh-oZ0w"
      },
      "outputs": [],
      "source": [
        "# copy the dataset from github folder to 'content' \n",
        "!cp -r '/content/amoeba-detection/dataset-level1/dataset-amoebaClassification' '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efgc0DXyRfeR"
      },
      "outputs": [],
      "source": [
        "# upload zip file of the dataset from local\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for fn in uploaded.keys():\n",
        "#     print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# !unzip dataset-amoebaClassification.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpenRg5MT2IY"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0zjFJ_TXoS1"
      },
      "source": [
        "In this step, we will prepare dataset, including data augmentation, spliting training, validation and test dataset, and normalize dataset. More way to prepare the dataset can be found here: https://keras.io/api/data_loading/image/. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation"
      ],
      "metadata": {
        "id": "MxsVk8Bhe-yI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data augmentation can obtain more data for training and validation. When the training dataset is small, we can do data augmentation to existing data and add those data to the training dataset. For image data specifically, data augmentation could consist of things like flipping the image horizontally or vertically, rotating the image, zooming in or out, cropping, or varying the color and so on. "
      ],
      "metadata": {
        "id": "I58yqNoCfl8u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3MVty6exIs1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image\n",
        "\n",
        "gen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, \n",
        "    channel_shift_range=10., horizontal_flip=True)\n",
        "    \n",
        "# Data augmentation for train amoeba dataset\n",
        "fileList=os.listdir(\"./dataset-amoebaClassification/train/amoeba/\")\n",
        "\n",
        "for i in fileList:\n",
        "    image_path = 'dataset-amoebaClassification/train/amoeba/' + i\n",
        "    image = np.expand_dims(plt.imread(image_path),0)\n",
        "    aug_iter = gen.flow(image)\n",
        "    aug_images = [next(aug_iter)[0].astype(np.uint8) for m in range(2)]\n",
        "\n",
        "    for j in range(0,2):\n",
        "        img = Image.fromarray(aug_images[j])\n",
        "        save_path = './dataset-amoebaClassification/train/amoeba' + \"/\" + \"aug-\" + str(j) + str(i)\n",
        "        img.save(save_path)\n",
        "\n",
        "# The augmentation image will save into ./dataset-amoebaClassification/train/amoeba folder. Check this folder and find out\\\n",
        "# augmentation image with prefix 'aug-'. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC1EmQi1xLKf"
      },
      "outputs": [],
      "source": [
        "# Data augmentation for train noAmoeba dataset\n",
        "fileList2=os.listdir(\"./dataset-amoebaClassification/train/noAmoeba/\")\n",
        "\n",
        "for i2 in fileList2:\n",
        "    image_path = 'dataset-amoebaClassification/train/noAmoeba/' + i2\n",
        "    image = np.expand_dims(plt.imread(image_path),0)\n",
        "    aug_iter = gen.flow(image)\n",
        "    aug_images = [next(aug_iter)[0].astype(np.uint8) for m2 in range(2)]\n",
        "\n",
        "    for j2 in range(0,2):\n",
        "        img = Image.fromarray(aug_images[j2])\n",
        "        save_path = './dataset-amoebaClassification/train/noAmoeba' + \"/\" + \"aug-\" + str(j2) + str(i2)\n",
        "        img.save(save_path)\n",
        "\n",
        "# The augmentation image will save into ./dataset-amoebaClassification/train/noAmoeba folder. Check this folder and find out\\\n",
        "# augmentation image with prefix 'aug-'. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation for test Amoeba dataset\n",
        "fileList3=os.listdir(\"./dataset-amoebaClassification/test/amoeba/\")\n",
        "\n",
        "for i3 in fileList3:\n",
        "    image_path = 'dataset-amoebaClassification/test/amoeba/' + i3\n",
        "    image = np.expand_dims(plt.imread(image_path),0)\n",
        "    aug_iter = gen.flow(image)\n",
        "    aug_images = [next(aug_iter)[0].astype(np.uint8) for m3 in range(2)]\n",
        "\n",
        "    for j3 in range(0,2):\n",
        "        img = Image.fromarray(aug_images[j3])\n",
        "        save_path = './dataset-amoebaClassification/test/amoeba' + \"/\" + \"aug-\" + str(j3) + str(i3)\n",
        "        img.save(save_path)\n",
        "\n",
        "# The augmentation image will save into ./dataset-amoebaClassification/test/amoeba folder. Check this folder and find out\\\n",
        "# augmentation image with prefix 'aug-'. "
      ],
      "metadata": {
        "id": "LIqD_XJew8tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation for test noAmoeba dataset\n",
        "fileList4=os.listdir(\"./dataset-amoebaClassification/test/noAmoeba/\")\n",
        "\n",
        "for i4 in fileList4:\n",
        "    image_path = 'dataset-amoebaClassification/test/noAmoeba/' + i4\n",
        "    image = np.expand_dims(plt.imread(image_path),0)\n",
        "    aug_iter = gen.flow(image)\n",
        "    aug_images = [next(aug_iter)[0].astype(np.uint8) for m4 in range(2)]\n",
        "\n",
        "    for j4 in range(0,2):\n",
        "        img = Image.fromarray(aug_images[j4])\n",
        "        save_path = './dataset-amoebaClassification/test/noAmoeba' + \"/\" + \"aug-\" + str(j4) + str(i4)\n",
        "        img.save(save_path)\n",
        "\n",
        "# The augmentation image will save into ./dataset-amoebaClassification/test/noAmoeba folder. Check this folder and find out\\\n",
        "# augmentation image with prefix 'aug-'. "
      ],
      "metadata": {
        "id": "KGCZg-KDw-9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split and normalize the dataset"
      ],
      "metadata": {
        "id": "DnpB0EcEg5QA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxTtsppveenE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "\n",
        "# training data preparation\n",
        "\n",
        "# define image size, it can be modified \n",
        "img_size = 150\n",
        "# training images dataset path\n",
        "train_path = 'dataset-amoebaClassification/train' \n",
        "nub_train = len(glob(train_path + '/*/*.jpg'))\n",
        "# Create empty array, fill out the image array to newly-created array. \n",
        "X_train_full = np.zeros((nub_train,img_size,img_size,3),dtype=np.uint8) \n",
        "y_train_full = np.zeros((nub_train,),dtype=np.uint8)\n",
        "\n",
        "i = 0\n",
        "for img_path in tqdm(glob(train_path + '/*/*.jpg')):\n",
        "    img = Image.open(img_path)\n",
        "    # image resize\n",
        "    img = img.resize((img_size,img_size)) \n",
        "    # images are converted to array\n",
        "    arr = np.asarray(img)\n",
        "    # assign array\n",
        "    X_train_full[i, :, :, :] = arr\n",
        "    \n",
        "    if img_path.split('/')[-2] == 'amoeba':\n",
        "        # Set amoeba class as 1\n",
        "        y_train_full[i] = 1\n",
        "    else:\n",
        "        # Set no amoeba class as 0\n",
        "        y_train_full[i] = 0\n",
        "        \n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6b2hvOceltB"
      },
      "outputs": [],
      "source": [
        "# # test data preparation\n",
        "\n",
        "img_size = 150\n",
        "test_path = 'dataset-amoebaClassification/test'\n",
        "nub_test = len(glob(test_path + '/*/*.jpg'))\n",
        "\n",
        "X_test = np.zeros((nub_test,img_size,img_size,3),dtype=np.uint8) \n",
        "y_test = np.zeros((nub_test,),dtype=np.uint8)\n",
        "\n",
        "i = 0\n",
        "for img_path in tqdm(glob(test_path + '/*/*.jpg')):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((img_size,img_size))\n",
        "    arr = np.asarray(img)\n",
        "    X_test[i, :, :, :] = arr\n",
        "          \n",
        "    if img_path.split('/')[-2] == 'amoeba':\n",
        "        # Set amoeba class as 1\n",
        "        y_test[i] = 1\n",
        "    else:\n",
        "        # Set no amoeba class as 0\n",
        "        y_test[i] = 0\n",
        "        \n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QDAA7mieoAH"
      },
      "outputs": [],
      "source": [
        "# # Visualize the training dataset\n",
        "fig,axes = plt.subplots(3,4,figsize=(20, 20))\n",
        "\n",
        "j = 0\n",
        "for i,img in enumerate(X_train_full[:12]):\n",
        "    axes[i//4,j%4].imshow(img)\n",
        "    j+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ksJIfJSrRA6"
      },
      "outputs": [],
      "source": [
        "# normalize the dataset\n",
        "X_mean = X_train_full.mean(axis=0, keepdims=True)\n",
        "X_std = X_train_full.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train_full_norm = (X_train_full - X_mean) / X_std\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train_full_norm = X_train_full_norm[..., np.newaxis]\n",
        "X_test_norm = X_test_norm[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO1wdMbJ3cYe"
      },
      "outputs": [],
      "source": [
        "# Split the X_train_full_norm and y_train_full dataset randomly\n",
        "def split_dataset(X_train_full_norm, y_train_full):\n",
        "    # randomize X_train_full_norm and y_train_full array at the same order\n",
        "    total_images = len(X_train_full_norm)\n",
        "    idx = np.random.choice(np.arange(total_images), total_images, replace=False)\n",
        "    X_train_full_norm = X_train_full_norm[idx]\n",
        "    y_train_full = y_train_full[idx]\n",
        "\n",
        "    # split the X_train_full into X_train(3/4) and X_valid(1/4)\n",
        "    X_train_norm, X_validation_norm = X_train_full_norm[0 : int(total_images*3/4)], X_train_full_norm[int(total_images*3/4):total_images]\n",
        "    y_train, y_validation = y_train_full[0 : int(total_images*3/4)], y_train_full[int(total_images*3/4):total_images]\n",
        "    return X_train_norm, X_validation_norm, y_train, y_validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Hy73CASCOy"
      },
      "source": [
        "# Build and train the model (blank in here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odOHP35HCpJl"
      },
      "source": [
        "Simple convolustional neural network (CNN) is used to train the model. It is usually composed of different layers in keras. The Layers are the basic building blocks of neural networks in Keras. Those layers are composed of and can be referred in Keras doc: convolutional layers (https://keras.io/api/layers/convolution_layers/convolution2d/), pooling layer (https://keras.io/api/layers/pooling_layers/), input and dense layer (https://keras.io/api/layers/core_layers/), flatten layer (https://keras.io/api/layers/reshaping_layers/flatten/). The layer or CNN structure can refer to example in keras doc ( https://keras.io/examples/vision/mnist_convnet/) or related books (such as •\tGéron, A., 2019. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\".). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbT_kEdWrZwU"
      },
      "outputs": [],
      "source": [
        "# Build the model (sequential CNN model)\n",
        "from functools import partial\n",
        "\n",
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "\n",
        "model = keras.models.Sequential([                           \n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[150, 150, 3]),  # input shape should match the input image size\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=32, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=2, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training. For more available loss function, refer keras doc: https://keras.io/api/losses/\n",
        "\n",
        "An optimizer is a function that modifies the attributes of the neural network, such as weights and learning rate. Thus, it helps in reducing the overall loss and improve the accuracy. For more available optimizer, refer keras doc: https://keras.io/api/optimizers/\n",
        "\n",
        "A metric is a function that is used to judge the performance of your model. For more available metrics, refer keras doc: https://keras.io/api/metrics/"
      ],
      "metadata": {
        "id": "sON4dD3_qxVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUmjbFiJrcga"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "opt = keras.optimizers.Nadam(learning_rate=0.00001)  # learning rate can be changed to increase the performance of the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will start to train the model by calling `model.fit()` and save the model by calling `model.save()`. More information about these two functions are in the Keras doc: https://keras.io/api/models/. "
      ],
      "metadata": {
        "id": "vO7tdsbqXJUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "! fill out the blank here"
      ],
      "metadata": {
        "id": "780xOAYZjupS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04XEgAhtk1Na"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "# Create a dir to save our model\n",
        "if not os.path.exists(\"model_dir\"):\n",
        "  os.mkdir(\"model_dir\")\n",
        "\n",
        "repeat_exp = 11  # Set how many times (repeat_exp - 1) you want to repeat the experiment\n",
        "for i in range(1, repeat_exp): \n",
        "    # Split the dataset randomly\n",
        "    X_train_norm, X_validation_norm, y_train, y_validation = split_dataset(X_train_full_norm, y_train_full)\n",
        "\n",
        "    # rebuild the model\n",
        "    DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "    model = keras.models.Sequential([                           \n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[150, 150, 3]),  # input shape should match the input image size\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=32, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=2, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    opt = keras.optimizers.Nadam(learning_rate=0.00001)  # learning rate can be changed to increase the performance of the model\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "    # Start to train the model\n",
        "    print(\"start repeatation \", i)\n",
        "    history = model.fit( , , epochs=10, validation_data=(X_validation_norm, y_validation)) # fill out the blank\n",
        "    # Save the trained model in model_dir dir\n",
        "    saved_model = \"model_dir/\" + \"model\" + str(i)\n",
        "    model.save(saved_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw8CUGhuSQN5"
      },
      "source": [
        "# Evaluate the model (blank in here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGy1zgY4e7Hk"
      },
      "outputs": [],
      "source": [
        "# visualize the model structure with model.summary(). Feel free to comment out the code below to visualize the model structure\n",
        "\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZU4WzhMDZx-U"
      },
      "source": [
        "The test dataset is not used for training and validation, which means they are new to the trained model. And we will use it to get the performance of the model. The performance is acceptable because the accuracy for test dataset is nearly the same with accuracy for train and validation dataset.\n",
        "\n",
        "Another way to evaluate the small dataset is to repeat the experiment of evaluating the model multiple times. And we can calculate mean test accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "! fill out the blank here"
      ],
      "metadata": {
        "id": "VfcVGxgwj7BD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GL_qK5fgMTCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbda4d2f-45aa-4429-97cd-b737a1290ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step - loss: 0.2340 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 0.1244 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 0.0739 - accuracy: 1.0000\n",
            "WARNING:tensorflow:5 out of the last 604 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8c52df0dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.0893 - accuracy: 1.0000\n",
            "WARNING:tensorflow:6 out of the last 605 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8d305869e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 0.0995 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 248ms/step - loss: 0.1601 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 270ms/step - loss: 0.1529 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 0.0983 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 255ms/step - loss: 0.0623 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 0.1079 - accuracy: 1.0000\n",
            "mean test accuracy:  1.0\n"
          ]
        }
      ],
      "source": [
        "import os   \n",
        "fileList = os.listdir(\"./model_dir/\")\n",
        "\n",
        "test_accuracy_list = list()\n",
        "loss_list = list()\n",
        "for m in fileList:\n",
        "    model = keras.models.load_model(\"./model_dir/\"+m) # m='model.h5'\n",
        "    results = model.evaluate( ,  , batch_size=128)  # fill out the blank here\n",
        "    test_accuracy_list.append(results[1])\n",
        "    loss_list.append(results[0])\n",
        "\n",
        "mean_test_accuracy = np.mean(test_accuracy_list)\n",
        "print(\"mean test accuracy: \", mean_test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find out the best model among all trained models."
      ],
      "metadata": {
        "id": "-PIJo7ZYjvsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pt3fzKXQMZnP"
      },
      "outputs": [],
      "source": [
        "# find the best model\n",
        "for a,b,c in zip(fileList, test_accuracy_list, loss_list):\n",
        "    maxb = max(test_accuracy_list)\n",
        "    minc = min(loss_list)\n",
        "    if b == maxb and c==minc:\n",
        "        model = keras.models.load_model(\"./model_dir/\"+a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axn1bFdOZwIu"
      },
      "source": [
        "At the end of epoch, the accuary for training and validation dataset should be close. This is an easy way to determine if there is overfitting or not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4uf0r4ie9Ow"
      },
      "outputs": [],
      "source": [
        "# plot accuracy vs epoch\n",
        "plt.plot(history.history['accuracy'],'r')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left') \n",
        "plt.ylim([0, 1.1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7nuvpvtULUK"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql-ieznSZ6I0"
      },
      "source": [
        "We will run inference using the best model we find.\n",
        "\n",
        "We visualize the image and use your judgement to see if there is amoeba exist in the image and then compare it with the model prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llMoMhl3jsKu"
      },
      "outputs": [],
      "source": [
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# Visualize one image, X_test[x]. Here we choose X_test[1]. You can choose any of the images among all test dataset\n",
        "inference_image_number = 1  # Choose the number from 0-66 (amount of test dataset)!\n",
        "img1 = X_test[inference_image_number]\n",
        "plt.imshow(img1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpB2C5ccZ_r0"
      },
      "source": [
        "We will predict the image above with `model.predict()` function to see if it matches with your judgement. Check the `model.predict()` reference: https://keras.io/api/models/model_training_apis/#predict-method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpCYQiw5j-Y_"
      },
      "outputs": [],
      "source": [
        "# class label\n",
        "class_label = ['no amoeba exists', 'amoeba exists']\n",
        "\n",
        "# image process\n",
        "x = np.squeeze(X_test_norm[inference_image_number])\n",
        "x = image.img_to_array(x)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# predict the image with model.predict()\n",
        "y_prob = model.predict(x)\n",
        "print(\"probability for each of the catogaries: \", y_prob)\n",
        "y_class = y_prob.argmax(axis=-1)\n",
        "print(\"model predict: \", class_label[y_class[0]])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_AmoebaClassification.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}